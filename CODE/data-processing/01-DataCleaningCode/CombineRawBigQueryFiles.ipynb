{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Raw Big Query Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes in zip file folders of Reddit comments and/or Reddit submissions as directly downloaded from the Google Big Query data source in order to output smaller output files that only contain the subreddits of interest to be used in analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get your data.\n",
    "\n",
    "Files and folders will begin with:\n",
    "- RC_YYYY_MM for reddit comments  \n",
    "- RS_YYYY_MM for reddit submissions (posts)\n",
    "\n",
    "Your files should be stored as zip files (**DO NOT** manually unzip files) with the following hierarchy:\n",
    "\n",
    "**raw_data_folder**  \n",
    "$\\hspace{10mm}$**Rx_YYYY_MM**   <- (folder)  \n",
    "$\\hspace{20mm}$**Rx_YYYY_MM_docXX.gz**   <- (gzipped csv file)  \n",
    "$\\hspace{20mm}$**Rx_YYYY_MM_docXX.gz**   <- (gzipped csv file)  \n",
    "$\\hspace{20mm}$...  \n",
    "$\\hspace{10mm}$**Rx_YYYY_MM** <- (folder)  \n",
    "$\\hspace{20mm}$**Rx_YYYY_MM_docXX.gz**   <- (gzipped csv file)  \n",
    "$\\hspace{20mm}$**Rx_YYYY_MM_docXX.gz**   <- (gzipped csv file)  \n",
    "$\\hspace{20mm}$...  \n",
    "$\\hspace{10mm}$...   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import shutil\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def get_immediate_subdirectories(a_dir):\n",
    "    subdirectories = [name for name in os.listdir(a_dir) \n",
    "                      if os.path.isdir(os.path.join(a_dir, name)) \n",
    "                      and not name.startswith('.')]\n",
    "    return sorted(subdirectories)\n",
    "\n",
    "def get_files_in_dir(a_dir):\n",
    "    return sorted([os.path.join(a_dir,f) for f in os.listdir(a_dir) if not f.startswith('.')])\n",
    "\n",
    "def get_filepaths_in_folders(raw_data_folder):\n",
    "    infolders = get_immediate_subdirectories(raw_data_folder)\n",
    "\n",
    "    filepaths = {folder:get_files_in_dir(os.path.join(raw_data_folder,folder)) for folder in infolders}\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files(raw_data_folder, outfolder, preview):\n",
    "    \n",
    "    ### WARNING: this will overwrite any existing folders/files of the same name as the output folder\n",
    "    start = time.time()\n",
    "    \n",
    "    print(round(time.time()-start),'seconds','\\tGetting file names')\n",
    "    files = get_filepaths_in_folders(raw_data_folder)\n",
    "    \n",
    "    if not os.path.exists(outfolder):\n",
    "        os.makedirs(outfolder)\n",
    "        \n",
    "    subreddits = ['canada', 'politics', 'Republican', 'The_Donald', 'Catholicism', \n",
    "              'news', 'worldnews', 'Libertarian', 'PoliticalDiscussion', 'communism',\n",
    "              'unitedkingdom', 'TrueReddit', 'Conservative', 'college', 'socialism',\n",
    "              'TheNewRight', 'geopolitics', 'technology', 'environment', 'neutralnews',\n",
    "              'Anarchism', 'NeutralPolitics', 'worldpolitics', 'democrats', 'Liberal',\n",
    "              'progressive', 'Full_news', 'moderatepolitics', 'qualitynews', 'worldevents', \n",
    "              'internationalpolitics']\n",
    "        \n",
    "    \n",
    "    for folder in files:\n",
    "        print('\\n----- Folder: {} -----'.format(folder))\n",
    "        if not os.path.exists(os.path.join(outfolder,folder)):\n",
    "            os.makedirs(os.path.join(outfolder,folder))\n",
    "        print(round(time.time()-start),'seconds','\\tReading files in folder')\n",
    "        \n",
    "        if folder[0:2] == 'RC':\n",
    "            columns = ['body', 'subreddit', 'link_id']\n",
    "        elif folder[0:2] == 'RS':\n",
    "            columns = ['subreddit', 'id','title','domain', 'url','selftext', 'is_self']\n",
    "            #title,subreddit,created_utc,num_comments,score,selftext,id,domain,url,is_self,permalink\n",
    "\n",
    "        df = pd.concat((pd.read_csv(file, compression='gzip', header=0, sep=',', quotechar='\"',\n",
    "                        usecols = columns) \n",
    "                        for file in files[folder]))\n",
    "\n",
    "        \n",
    "        for sub in subreddits:\n",
    "            outfile_name = str(folder)+'_'+ str(sub) +'.csv'\n",
    "            print(round(time.time()-start),'seconds','\\tWriting',outfile_name)\n",
    "            \n",
    "            subdf = df[df['subreddit'] == sub]\n",
    "            subdf = subdf.replace('ï¿½','',regex=True)\n",
    "            subdf = subdf.replace('\"',\"'\",regex=True)\n",
    "            subdf = subdf.replace('&gt;','>',regex=True)\n",
    "            subdf = subdf.replace('&lt;','>', regex=True)\n",
    "            subdf = subdf.replace('&amp;','&', regex=True) \n",
    "            subdf = subdf.replace('&nbsp;','&', regex=True)\n",
    "            subdf = subdf.replace('&#x200b;','&', regex=True)\n",
    "            subdf = subdf.replace('\\s+',' ',regex=True)\n",
    "            subdf = subdf.replace('\\n*','',regex=True)\n",
    "            subdf = subdf.replace('\\[.+\\]\\([^\\s]+\\)','',regex=True)\n",
    "            subdf = subdf.replace('http\\w*:[^\\s]*','',regex=True)\n",
    "            subdf = subdf.replace('& ','',regex=True)\n",
    "            subdf = subdf.replace('> ','', regex=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if preview == True:\n",
    "                display(subdf.head(15))\n",
    "            f = os.path.join(outfolder,folder,outfile_name)\n",
    "            subdf.to_csv(f, header=True, index=False,quotechar='\"')   \n",
    "            \n",
    "            if os.path.getsize(f)/1000000 > 100:\n",
    "                print(round(time.time()-start),'seconds','\\tFile {} is too large (>100MB).  Splitting into multiple documents.'.format(outfile_name))\n",
    "                maxlines = 400000\n",
    "                with open(f) as f_in:\n",
    "                    i = 0\n",
    "                    docnum = 0\n",
    "                    outfile_name = str(folder)+'_'+ str(sub) + '_doc' + str(docnum) +'.csv'\n",
    "                    f_out = open(os.path.join(outfolder,folder,outfile_name),'w')\n",
    "                    for line in f_in:\n",
    "                        f_out.write(line)\n",
    "                        i += 1\n",
    "                        if i%maxlines ==0 and i != 0:\n",
    "                            f_out.close()\n",
    "                            docnum += 1\n",
    "                            outfile_name = str(folder)+'_'+ str(sub) + '_doc' + str(docnum) +'.csv'\n",
    "                            f_out = open(os.path.join(outfolder,folder,outfile_name),'w')\n",
    "                    f_out.close()\n",
    "                os.remove(f) \n",
    "        \n",
    "        end = time.time()\n",
    "        print('\\n{} minutes {} seconds'.format(floor((end-start)/60), round((end-start)%60)))\n",
    "        \n",
    "        \n",
    "    print('\\nscript complete')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fill out all variables in this cell\n",
    "\n",
    "# #set path to the raw BigQuery data folder\n",
    "raw_data_folder = '../00-GettingData/ExampleRawBigQueryFiles'\n",
    "\n",
    "#within the github folder, set the output folder where you would like output files to be saved\n",
    "# ##### WARNING: if the folders/files already exist, the function will overwrite existing files! #####\n",
    "outfolder = 'ParsedBigQueryData'\n",
    "\n",
    "#do you want the script to show previews of the output data?\n",
    "preview = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 seconds \tGetting file names\n",
      "\n",
      "----- Folder: RC_2018_12 -----\n",
      "0 seconds \tReading files in folder\n",
      "23 seconds \tWriting RC_2018_12_canada.csv\n",
      "39 seconds \tWriting RC_2018_12_politics.csv\n",
      "181 seconds \tFile RC_2018_12_politics.csv is too large (>100MB).  Splitting into multiple documents.\n",
      "183 seconds \tWriting RC_2018_12_Republican.csv\n",
      "184 seconds \tWriting RC_2018_12_The_Donald.csv\n",
      "231 seconds \tFile RC_2018_12_The_Donald.csv is too large (>100MB).  Splitting into multiple documents.\n",
      "232 seconds \tWriting RC_2018_12_Catholicism.csv\n",
      "238 seconds \tWriting RC_2018_12_news.csv\n",
      "288 seconds \tFile RC_2018_12_news.csv is too large (>100MB).  Splitting into multiple documents.\n",
      "289 seconds \tWriting RC_2018_12_worldnews.csv\n",
      "363 seconds \tFile RC_2018_12_worldnews.csv is too large (>100MB).  Splitting into multiple documents.\n",
      "364 seconds \tWriting RC_2018_12_Libertarian.csv\n",
      "378 seconds \tWriting RC_2018_12_PoliticalDiscussion.csv\n",
      "381 seconds \tWriting RC_2018_12_communism.csv\n",
      "382 seconds \tWriting RC_2018_12_unitedkingdom.csv\n",
      "392 seconds \tWriting RC_2018_12_TrueReddit.csv\n",
      "394 seconds \tWriting RC_2018_12_Conservative.csv\n",
      "399 seconds \tWriting RC_2018_12_college.csv\n",
      "401 seconds \tWriting RC_2018_12_socialism.csv\n",
      "403 seconds \tWriting RC_2018_12_TheNewRight.csv\n",
      "404 seconds \tWriting RC_2018_12_geopolitics.csv\n",
      "406 seconds \tWriting RC_2018_12_technology.csv\n",
      "417 seconds \tWriting RC_2018_12_environment.csv\n",
      "419 seconds \tWriting RC_2018_12_neutralnews.csv\n",
      "420 seconds \tWriting RC_2018_12_Anarchism.csv\n",
      "422 seconds \tWriting RC_2018_12_NeutralPolitics.csv\n",
      "423 seconds \tWriting RC_2018_12_worldpolitics.csv\n",
      "427 seconds \tWriting RC_2018_12_democrats.csv\n",
      "428 seconds \tWriting RC_2018_12_Liberal.csv\n",
      "428 seconds \tWriting RC_2018_12_progressive.csv\n",
      "428 seconds \tWriting RC_2018_12_Full_news.csv\n",
      "429 seconds \tWriting RC_2018_12_moderatepolitics.csv\n",
      "430 seconds \tWriting RC_2018_12_qualitynews.csv\n",
      "430 seconds \tWriting RC_2018_12_worldevents.csv\n",
      "431 seconds \tWriting RC_2018_12_internationalpolitics.csv\n",
      "\n",
      "7 minutes 11 seconds\n",
      "\n",
      "----- Folder: RS_2018_12 -----\n",
      "431 seconds \tReading files in folder\n",
      "433 seconds \tWriting RS_2018_12_canada.csv\n",
      "433 seconds \tWriting RS_2018_12_politics.csv\n",
      "436 seconds \tWriting RS_2018_12_Republican.csv\n",
      "436 seconds \tWriting RS_2018_12_The_Donald.csv\n",
      "442 seconds \tWriting RS_2018_12_Catholicism.csv\n",
      "442 seconds \tWriting RS_2018_12_news.csv\n",
      "445 seconds \tWriting RS_2018_12_worldnews.csv\n",
      "449 seconds \tWriting RS_2018_12_Libertarian.csv\n",
      "450 seconds \tWriting RS_2018_12_PoliticalDiscussion.csv\n",
      "450 seconds \tWriting RS_2018_12_communism.csv\n",
      "450 seconds \tWriting RS_2018_12_unitedkingdom.csv\n",
      "450 seconds \tWriting RS_2018_12_TrueReddit.csv\n",
      "451 seconds \tWriting RS_2018_12_Conservative.csv\n",
      "451 seconds \tWriting RS_2018_12_college.csv\n",
      "452 seconds \tWriting RS_2018_12_socialism.csv\n",
      "452 seconds \tWriting RS_2018_12_TheNewRight.csv\n",
      "452 seconds \tWriting RS_2018_12_geopolitics.csv\n",
      "452 seconds \tWriting RS_2018_12_technology.csv\n",
      "453 seconds \tWriting RS_2018_12_environment.csv\n",
      "453 seconds \tWriting RS_2018_12_neutralnews.csv\n",
      "453 seconds \tWriting RS_2018_12_Anarchism.csv\n",
      "454 seconds \tWriting RS_2018_12_NeutralPolitics.csv\n",
      "454 seconds \tWriting RS_2018_12_worldpolitics.csv\n",
      "454 seconds \tWriting RS_2018_12_democrats.csv\n",
      "455 seconds \tWriting RS_2018_12_Liberal.csv\n",
      "455 seconds \tWriting RS_2018_12_progressive.csv\n",
      "455 seconds \tWriting RS_2018_12_Full_news.csv\n",
      "455 seconds \tWriting RS_2018_12_moderatepolitics.csv\n",
      "455 seconds \tWriting RS_2018_12_qualitynews.csv\n",
      "455 seconds \tWriting RS_2018_12_worldevents.csv\n",
      "455 seconds \tWriting RS_2018_12_internationalpolitics.csv\n",
      "\n",
      "7 minutes 35 seconds\n",
      "\n",
      "script complete\n"
     ]
    }
   ],
   "source": [
    "##run this cell as-is\n",
    "parse_files(raw_data_folder, outfolder, preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
